{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Tharneshtharun69/Python/blob/main/PROJECT%20TESTING%20CNN%20AND%20RNN\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "# Step 1: Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Step 2: Specify the folder path\n",
        "# Update 'your-folder-path' with the path to your folder in Google Drive.\n",
        "folder_path = '/content/drive/My Drive/FINAL_CODE_EMO_R_A ORIGINAL - Copy'\n",
        "\n",
        "# Step 3: Check and load files from the folder\n",
        "if os.path.exists(folder_path):\n",
        "    print(f\"Folder '{folder_path}' contents:\")\n",
        "    for file_name in os.listdir(folder_path):\n",
        "        print(file_name)\n",
        "else:\n",
        "    print(f\"The folder '{folder_path}' does not exist.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bh4gvYXn1IZw",
        "outputId": "dd93bd36-d3db-4e0f-9b65-4c19794390c3"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Folder '/content/drive/My Drive/FINAL_CODE_EMO_R_A ORIGINAL - Copy' contents:\n",
            "RNN\n",
            "CNN\n",
            "DATASET\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "# Step 1: Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Step 2: Specify the folder path in Google Drive\n",
        "# Replace 'your-folder-path' with the actual folder path in your Google Drive\n",
        "folder_path = '/content/drive/My Drive/FINAL_CODE_EMO_R_A ORIGINAL - Copy'\n",
        "\n",
        "# Step 3: Check and list all files in the folder\n",
        "if os.path.exists(folder_path):\n",
        "    print(f\"Files in '{folder_path}':\")\n",
        "    for file_name in os.listdir(folder_path):\n",
        "        file_path = os.path.join(folder_path, file_name)\n",
        "        print(file_path)  # Print the full path of each file\n",
        "\n",
        "        # Example: Load each file based on its type\n",
        "        if file_name.endswith('.csv'):\n",
        "            import pandas as pd\n",
        "            data = pd.read_csv(file_path)\n",
        "            print(f\"Loaded CSV file: {file_name}\")\n",
        "            print(data.head())  # Display first few rows\n",
        "\n",
        "        elif file_name.endswith(('.jpg', '.png')):\n",
        "            from PIL import Image\n",
        "            image = Image.open(file_path)\n",
        "            print(f\"Loaded Image file: {file_name}\")\n",
        "            # Display image size as an example\n",
        "            print(f\"Image size: {image.size}\")\n",
        "\n",
        "        elif file_name.endswith('.txt'):\n",
        "            with open(file_path, 'r') as file:\n",
        "                content = file.read()\n",
        "                print(f\"Loaded Text file: {file_name}\")\n",
        "                print(content[:100])  # Display first 100 characters\n",
        "\n",
        "else:\n",
        "    print(f\"The folder '{folder_path}' does not exist.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t4KYNDSv2nzd",
        "outputId": "02812a5e-2ff2-480c-9289-3d2126852d56"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Files in '/content/drive/My Drive/FINAL_CODE_EMO_R_A ORIGINAL - Copy':\n",
            "/content/drive/My Drive/FINAL_CODE_EMO_R_A ORIGINAL - Copy/RNN\n",
            "/content/drive/My Drive/FINAL_CODE_EMO_R_A ORIGINAL - Copy/CNN\n",
            "/content/drive/My Drive/FINAL_CODE_EMO_R_A ORIGINAL - Copy/DATASET\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "# Step 1: Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Step 2: Specify the file path\n",
        "# Replace 'your-file-path' with the actual path to the file in your Google Drive\n",
        "file_path = '/content/drive/My Drive/FINAL_CODE_EMO_R_A ORIGINAL - Copy/CNN/test.ipynb'\n",
        "\n",
        "# Step 3: Check if the file exists and open it\n",
        "if os.path.exists(file_path):\n",
        "    print(f\"Opening file: {file_path}\")\n",
        "\n",
        "    # Example: Handling the file based on its type\n",
        "    if file_path.endswith('.csv'):\n",
        "        import pandas as pd\n",
        "        data = pd.read_csv(file_path)\n",
        "        print(f\"Loaded CSV file:\\n{data.head()}\")\n",
        "\n",
        "    elif file_path.endswith(('.jpg', '.png')):\n",
        "        from PIL import Image\n",
        "        image = Image.open(file_path)\n",
        "        image.show()\n",
        "        print(f\"Image size: {image.size}\")\n",
        "\n",
        "    elif file_path.endswith('.txt'):\n",
        "        with open(file_path, 'r') as file:\n",
        "            content = file.read()\n",
        "            print(f\"Text file content:\\n{content[:500]}\")  # Display first 500 characters\n",
        "\n",
        "    else:\n",
        "        print(f\"File type not recognized for file: {file_path}\")\n",
        "else:\n",
        "    print(f\"The file '{file_path}' does not exist.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TodbGOAG2ukn",
        "outputId": "2946b8ba-09a0-4a7f-b9a2-fb05fe34b8cb"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Opening file: /content/drive/My Drive/FINAL_CODE_EMO_R_A ORIGINAL - Copy/CNN/test.ipynb\n",
            "File type not recognized for file: /content/drive/My Drive/FINAL_CODE_EMO_R_A ORIGINAL - Copy/CNN/test.ipynb\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#CNN\n",
        "import os\n",
        "import numpy as np\n",
        "import librosa\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "import joblib  # For saving and loading the label encoder\n",
        "\n",
        "# Function to load and preprocess audio data\n",
        "def load_data(dataset_path):\n",
        "    features = []  # List to store feature vectors\n",
        "    labels = []    # List to store corresponding labels\n",
        "    for emotion in os.listdir(dataset_path):\n",
        "        emotion_path = os.path.join(dataset_path, emotion)\n",
        "        if os.path.isdir(emotion_path):\n",
        "            for file in os.listdir(emotion_path):\n",
        "                if file.endswith('.wav'):\n",
        "                    file_path = os.path.join(emotion_path, file)\n",
        "                    audio, sr = librosa.load(file_path, sr=None)  # Load audio file\n",
        "                    mfcc = librosa.feature.mfcc(y=audio, sr=sr, n_mfcc=13)  # Extract MFCC features\n",
        "                    mfcc_scaled = np.mean(mfcc.T, axis=0)  # Average MFCC coefficients\n",
        "                    features.append(mfcc_scaled)\n",
        "                    labels.append(emotion)\n",
        "    return np.array(features), np.array(labels)\n",
        "\n",
        "# Define the path to your dataset\n",
        "dataset_path = r'/content/drive/My Drive/FINAL_CODE_EMO_R_A ORIGINAL - Copy/DATASET/Tess/'\n",
        "\n",
        "# Load the dataset\n",
        "X, y = load_data(dataset_path)\n",
        "\n",
        "# Encode the labels into numerical format\n",
        "label_encoder = LabelEncoder()\n",
        "y_encoded = label_encoder.fit_transform(y)\n",
        "\n",
        "# Split the dataset into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=42)\n",
        "\n",
        "# Reshape the data to fit the input requirements of a CNN\n",
        "X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)\n",
        "X_test = X_test.reshape(X_test.shape[0], X_test.shape[1], 1)\n",
        "\n",
        "# Build the CNN model using Conv1D layers\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Conv1D(32, kernel_size=3, activation='relu', input_shape=(X_train.shape[1], 1), padding='same'),\n",
        "    tf.keras.layers.MaxPooling1D(pool_size=2),\n",
        "    tf.keras.layers.Conv1D(64, kernel_size=3, activation='relu', padding='same'),\n",
        "    tf.keras.layers.MaxPooling1D(pool_size=2),\n",
        "    tf.keras.layers.Conv1D(128, kernel_size=3, activation='relu', padding='same'),\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dense(128, activation='relu'),\n",
        "    tf.keras.layers.Dense(len(label_encoder.classes_), activation='softmax')\n",
        "])\n",
        "\n",
        "# Compile the model with Adam optimizer and sparse categorical crossentropy loss\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Early stopping callback\n",
        "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
        "\n",
        "# Train the model on the training data\n",
        "history = model.fit(X_train, y_train, epochs=100, batch_size=32, validation_data=(X_test, y_test), callbacks=[early_stopping])\n",
        "\n",
        "# Save the trained model\n",
        "model.save('emotion_detection_model.h5')\n",
        "print('Model saved to disk.')\n",
        "\n",
        "# Save the label encoder\n",
        "label_encoder_path = 'label_encoder.pkl'\n",
        "joblib.dump(label_encoder, label_encoder_path)\n",
        "print(f'Label encoder saved to {label_encoder_path}.')\n",
        "\n",
        "# Evaluate the model on the test data\n",
        "y_pred = np.argmax(model.predict(X_test), axis=1)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f'Accuracy: {accuracy * 100:.2f}%')\n",
        "\n",
        "# Classification report\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test, y_pred, target_names=label_encoder.classes_))\n",
        "\n",
        "# Function to make predictions on new data\n",
        "def predict_emotion(file_path, model, label_encoder):\n",
        "    # Load the audio file\n",
        "    audio, sr = librosa.load(file_path, sr=None)\n",
        "    # Extract MFCC features\n",
        "    mfcc = librosa.feature.mfcc(y=audio, sr=sr, n_mfcc=13)\n",
        "    # Compute the mean of MFCC coefficients\n",
        "    mfcc_scaled = np.mean(mfcc.T, axis=0)\n",
        "    # Reshape the input to match the model's input shape\n",
        "    input_data = mfcc_scaled.reshape(1, mfcc_scaled.shape[0], 1)\n",
        "    # Make prediction\n",
        "    prediction = model.predict(input_data)\n",
        "    # Get the class with highest probability\n",
        "    predicted_class = np.argmax(prediction, axis=1)[0]\n",
        "    # Decode the class label\n",
        "    predicted_label = label_encoder.classes_[predicted_class]\n",
        "    return predicted_label\n",
        "\n",
        "# Example usage:\n",
        "# Replace 'path_to_new_audio_file.wav' with the path to an actual WAV file\n",
        "new_file_path = r'/content/drive/My Drive/FINAL_CODE_EMO_R_A ORIGINAL - Copy/DATASET/Tess/OAF_angry/OAF_back_angry.wav'\n",
        "predicted_emotion = predict_emotion(new_file_path, model, label_encoder)\n",
        "print(f'Predicted emotion: {predicted_emotion}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qw-n-4gL5Ryb",
        "outputId": "30ae85cf-f9e8-4077-e7ff-ab6dc5f6a410"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 33ms/step - accuracy: 0.4394 - loss: 5.5254 - val_accuracy: 0.9859 - val_loss: 0.0876\n",
            "Epoch 2/100\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8899 - loss: 0.2299 - val_accuracy: 0.9718 - val_loss: 0.0908\n",
            "Epoch 3/100\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 0.0304 - val_accuracy: 1.0000 - val_loss: 0.0060\n",
            "Epoch 4/100\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9955 - loss: 0.0179 - val_accuracy: 1.0000 - val_loss: 5.6398e-04\n",
            "Epoch 5/100\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 1.0000 - loss: 8.2841e-04 - val_accuracy: 1.0000 - val_loss: 5.7187e-04\n",
            "Epoch 6/100\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 4.1149e-04 - val_accuracy: 1.0000 - val_loss: 3.8761e-04\n",
            "Epoch 7/100\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 1.0000 - loss: 4.7209e-04 - val_accuracy: 1.0000 - val_loss: 1.7201e-04\n",
            "Epoch 8/100\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 2.0800e-04 - val_accuracy: 1.0000 - val_loss: 1.5253e-04\n",
            "Epoch 9/100\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 1.0000 - loss: 3.1242e-04 - val_accuracy: 1.0000 - val_loss: 1.2779e-04\n",
            "Epoch 10/100\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 2.1304e-04 - val_accuracy: 1.0000 - val_loss: 1.4618e-04\n",
            "Epoch 11/100\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 1.0000 - loss: 2.8674e-04 - val_accuracy: 1.0000 - val_loss: 4.7929e-04\n",
            "Epoch 12/100\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 3.6171e-04 - val_accuracy: 1.0000 - val_loss: 8.7826e-05\n",
            "Epoch 13/100\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 1.0000 - loss: 1.8918e-04 - val_accuracy: 1.0000 - val_loss: 8.7250e-05\n",
            "Epoch 14/100\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 1.0000 - loss: 1.6300e-04 - val_accuracy: 1.0000 - val_loss: 1.0661e-04\n",
            "Epoch 15/100\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 1.0558e-04 - val_accuracy: 1.0000 - val_loss: 7.4395e-05\n",
            "Epoch 16/100\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 1.0000 - loss: 8.5896e-05 - val_accuracy: 1.0000 - val_loss: 1.3432e-04\n",
            "Epoch 17/100\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 9.5182e-05 - val_accuracy: 1.0000 - val_loss: 6.6557e-05\n",
            "Epoch 18/100\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 4.6369e-05 - val_accuracy: 1.0000 - val_loss: 3.8164e-05\n",
            "Epoch 19/100\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 4.5122e-05 - val_accuracy: 1.0000 - val_loss: 3.1813e-05\n",
            "Epoch 20/100\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 3.5890e-05 - val_accuracy: 1.0000 - val_loss: 3.8280e-05\n",
            "Epoch 21/100\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 5.0346e-05 - val_accuracy: 1.0000 - val_loss: 3.3233e-05\n",
            "Epoch 22/100\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 2.8717e-05 - val_accuracy: 1.0000 - val_loss: 2.4969e-05\n",
            "Epoch 23/100\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 4.8751e-05 - val_accuracy: 1.0000 - val_loss: 2.0842e-05\n",
            "Epoch 24/100\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 1.0000 - loss: 2.3007e-05 - val_accuracy: 1.0000 - val_loss: 1.7434e-05\n",
            "Epoch 25/100\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 1.0000 - loss: 3.3463e-05 - val_accuracy: 1.0000 - val_loss: 1.6295e-05\n",
            "Epoch 26/100\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 1.0000 - loss: 2.6069e-05 - val_accuracy: 1.0000 - val_loss: 1.5122e-05\n",
            "Epoch 27/100\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 1.0000 - loss: 2.9891e-05 - val_accuracy: 1.0000 - val_loss: 1.4474e-05\n",
            "Epoch 28/100\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 1.0000 - loss: 2.2150e-05 - val_accuracy: 1.0000 - val_loss: 1.1923e-05\n",
            "Epoch 29/100\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 1.0000 - loss: 1.9683e-05 - val_accuracy: 1.0000 - val_loss: 1.0597e-05\n",
            "Epoch 30/100\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 1.0000 - loss: 1.1001e-05 - val_accuracy: 1.0000 - val_loss: 9.8867e-06\n",
            "Epoch 31/100\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 1.0000 - loss: 1.2492e-05 - val_accuracy: 1.0000 - val_loss: 9.6517e-06\n",
            "Epoch 32/100\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 1.0000 - loss: 1.9773e-05 - val_accuracy: 1.0000 - val_loss: 8.4027e-06\n",
            "Epoch 33/100\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 1.0000 - loss: 1.0625e-05 - val_accuracy: 1.0000 - val_loss: 7.6372e-06\n",
            "Epoch 34/100\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 1.0000 - loss: 9.5813e-06 - val_accuracy: 1.0000 - val_loss: 6.9540e-06\n",
            "Epoch 35/100\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 1.0000 - loss: 6.7656e-06 - val_accuracy: 1.0000 - val_loss: 6.7911e-06\n",
            "Epoch 36/100\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 1.0000 - loss: 8.7277e-06 - val_accuracy: 1.0000 - val_loss: 6.4789e-06\n",
            "Epoch 37/100\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 1.0000 - loss: 9.9877e-06 - val_accuracy: 1.0000 - val_loss: 5.9920e-06\n",
            "Epoch 38/100\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 1.0000 - loss: 6.7212e-06 - val_accuracy: 1.0000 - val_loss: 5.5824e-06\n",
            "Epoch 39/100\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 1.0000 - loss: 9.2445e-06 - val_accuracy: 1.0000 - val_loss: 5.0451e-06\n",
            "Epoch 40/100\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 1.0000 - loss: 1.3562e-05 - val_accuracy: 1.0000 - val_loss: 4.7329e-06\n",
            "Epoch 41/100\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 1.0000 - loss: 1.2838e-05 - val_accuracy: 1.0000 - val_loss: 4.3350e-06\n",
            "Epoch 42/100\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 1.0000 - loss: 6.1220e-06 - val_accuracy: 1.0000 - val_loss: 4.0529e-06\n",
            "Epoch 43/100\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 1.0000 - loss: 1.1700e-05 - val_accuracy: 1.0000 - val_loss: 3.9421e-06\n",
            "Epoch 44/100\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 1.0000 - loss: 4.6478e-06 - val_accuracy: 1.0000 - val_loss: 3.7894e-06\n",
            "Epoch 45/100\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 1.0000 - loss: 5.4576e-06 - val_accuracy: 1.0000 - val_loss: 3.4721e-06\n",
            "Epoch 46/100\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 1.0000 - loss: 7.2020e-06 - val_accuracy: 1.0000 - val_loss: 3.2572e-06\n",
            "Epoch 47/100\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 1.0000 - loss: 9.2026e-06 - val_accuracy: 1.0000 - val_loss: 3.0423e-06\n",
            "Epoch 48/100\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 3.8103e-06 - val_accuracy: 1.0000 - val_loss: 2.8542e-06\n",
            "Epoch 49/100\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 1.0000 - loss: 3.7913e-06 - val_accuracy: 1.0000 - val_loss: 2.9768e-06\n",
            "Epoch 50/100\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 1.0000 - loss: 4.5980e-06 - val_accuracy: 1.0000 - val_loss: 2.5587e-06\n",
            "Epoch 51/100\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 1.0000 - loss: 4.2248e-06 - val_accuracy: 1.0000 - val_loss: 2.4714e-06\n",
            "Epoch 52/100\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 1.0000 - loss: 2.9978e-06 - val_accuracy: 1.0000 - val_loss: 2.4143e-06\n",
            "Epoch 53/100\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 1.0000 - loss: 3.7656e-06 - val_accuracy: 1.0000 - val_loss: 2.3539e-06\n",
            "Epoch 54/100\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 1.0000 - loss: 6.5352e-06 - val_accuracy: 1.0000 - val_loss: 2.3304e-06\n",
            "Epoch 55/100\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 1.0000 - loss: 5.2075e-06 - val_accuracy: 1.0000 - val_loss: 2.0164e-06\n",
            "Epoch 56/100\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 1.0000 - loss: 2.6478e-06 - val_accuracy: 1.0000 - val_loss: 2.0618e-06\n",
            "Epoch 57/100\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 1.0000 - loss: 1.7527e-06 - val_accuracy: 1.0000 - val_loss: 2.0080e-06\n",
            "Epoch 58/100\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 1.0000 - loss: 2.2172e-06 - val_accuracy: 1.0000 - val_loss: 1.8015e-06\n",
            "Epoch 59/100\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 3.7228e-06 - val_accuracy: 1.0000 - val_loss: 1.8972e-06\n",
            "Epoch 60/100\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 2.7266e-06 - val_accuracy: 1.0000 - val_loss: 1.8872e-06\n",
            "Epoch 61/100\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 1.0000 - loss: 1.8271e-06 - val_accuracy: 1.0000 - val_loss: 1.6773e-06\n",
            "Epoch 62/100\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 1.4977e-06 - val_accuracy: 1.0000 - val_loss: 1.5749e-06\n",
            "Epoch 63/100\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 1.0000 - loss: 2.9913e-06 - val_accuracy: 1.0000 - val_loss: 1.8469e-06\n",
            "Epoch 64/100\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 1.0000 - loss: 2.7393e-06 - val_accuracy: 1.0000 - val_loss: 1.3835e-06\n",
            "Epoch 65/100\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 1.0000 - loss: 2.4852e-06 - val_accuracy: 1.0000 - val_loss: 1.4473e-06\n",
            "Epoch 66/100\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 1.0000 - loss: 1.7543e-06 - val_accuracy: 1.0000 - val_loss: 1.4808e-06\n",
            "Epoch 67/100\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 1.0000 - loss: 1.1865e-06 - val_accuracy: 1.0000 - val_loss: 1.5984e-06\n",
            "Epoch 68/100\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 1.3803e-06 - val_accuracy: 1.0000 - val_loss: 1.0712e-06\n",
            "Epoch 69/100\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 1.9824e-06 - val_accuracy: 1.0000 - val_loss: 1.5245e-06\n",
            "Epoch 70/100\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 1.6497e-06 - val_accuracy: 1.0000 - val_loss: 1.0611e-06\n",
            "Epoch 71/100\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 1.4419e-06 - val_accuracy: 1.0000 - val_loss: 1.1837e-06\n",
            "Epoch 72/100\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 1.0000 - loss: 1.7786e-06 - val_accuracy: 1.0000 - val_loss: 1.2072e-06\n",
            "Epoch 73/100\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 1.1686e-06 - val_accuracy: 1.0000 - val_loss: 9.7717e-07\n",
            "Epoch 74/100\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 1.9960e-06 - val_accuracy: 1.0000 - val_loss: 1.2794e-06\n",
            "Epoch 75/100\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 9.4511e-07 - val_accuracy: 1.0000 - val_loss: 1.1652e-06\n",
            "Epoch 76/100\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 6.9050e-07 - val_accuracy: 1.0000 - val_loss: 9.6373e-07\n",
            "Epoch 77/100\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 1.3938e-06 - val_accuracy: 1.0000 - val_loss: 1.1014e-06\n",
            "Epoch 78/100\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 1.2696e-06 - val_accuracy: 1.0000 - val_loss: 9.6037e-07\n",
            "Epoch 79/100\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 1.0000 - loss: 1.2241e-06 - val_accuracy: 1.0000 - val_loss: 9.6037e-07\n",
            "Epoch 80/100\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 1.0000 - loss: 1.0943e-06 - val_accuracy: 1.0000 - val_loss: 9.0665e-07\n",
            "Epoch 81/100\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 1.2505e-06 - val_accuracy: 1.0000 - val_loss: 7.8912e-07\n",
            "Epoch 82/100\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 1.0000 - loss: 1.0894e-06 - val_accuracy: 1.0000 - val_loss: 8.9321e-07\n",
            "Epoch 83/100\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 7.1752e-07 - val_accuracy: 1.0000 - val_loss: 9.0497e-07\n",
            "Epoch 84/100\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 1.0000 - loss: 5.2459e-07 - val_accuracy: 1.0000 - val_loss: 8.0591e-07\n",
            "Epoch 85/100\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 7.4263e-07 - val_accuracy: 1.0000 - val_loss: 8.2773e-07\n",
            "Epoch 86/100\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 1.2828e-06 - val_accuracy: 1.0000 - val_loss: 8.1262e-07\n",
            "Epoch 87/100\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 5.9015e-07 - val_accuracy: 1.0000 - val_loss: 7.7401e-07\n",
            "Epoch 88/100\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 1.0000 - loss: 6.2838e-07 - val_accuracy: 1.0000 - val_loss: 7.1692e-07\n",
            "Epoch 89/100\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 4.0668e-07 - val_accuracy: 1.0000 - val_loss: 7.4379e-07\n",
            "Epoch 90/100\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 4.8298e-07 - val_accuracy: 1.0000 - val_loss: 1.0342e-06\n",
            "Epoch 91/100\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 3.6009e-07 - val_accuracy: 1.0000 - val_loss: 6.7663e-07\n",
            "Epoch 92/100\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 5.2706e-07 - val_accuracy: 1.0000 - val_loss: 1.0107e-06\n",
            "Epoch 93/100\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 3.7112e-07 - val_accuracy: 1.0000 - val_loss: 6.4641e-07\n",
            "Epoch 94/100\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 3.9021e-07 - val_accuracy: 1.0000 - val_loss: 6.8167e-07\n",
            "Epoch 95/100\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 1.0000 - loss: 2.6360e-07 - val_accuracy: 1.0000 - val_loss: 6.4641e-07\n",
            "Epoch 96/100\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 3.1409e-07 - val_accuracy: 1.0000 - val_loss: 6.9845e-07\n",
            "Epoch 97/100\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 5.1898e-07 - val_accuracy: 1.0000 - val_loss: 5.6246e-07\n",
            "Epoch 98/100\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 3.9970e-07 - val_accuracy: 1.0000 - val_loss: 6.5144e-07\n",
            "Epoch 99/100\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 5.1949e-07 - val_accuracy: 1.0000 - val_loss: 5.9268e-07\n",
            "Epoch 100/100\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 3.5466e-07 - val_accuracy: 1.0000 - val_loss: 6.7159e-07\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model saved to disk.\n",
            "Label encoder saved to label_encoder.pkl.\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step\n",
            "Accuracy: 100.00%\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "   OAF_angry       1.00      1.00      1.00        31\n",
            "     YAF_sad       1.00      1.00      1.00        40\n",
            "\n",
            "    accuracy                           1.00        71\n",
            "   macro avg       1.00      1.00      1.00        71\n",
            "weighted avg       1.00      1.00      1.00        71\n",
            "\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
            "Predicted emotion: OAF_angry\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#RNN\n",
        "import os\n",
        "import numpy as np\n",
        "import librosa\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "import joblib  # For saving and loading the label encoder\n",
        "\n",
        "# Function to load and preprocess audio data\n",
        "def load_data(dataset_path):\n",
        "    features = []  # List to store feature vectors\n",
        "    labels = []    # List to store corresponding labels\n",
        "    for emotion in os.listdir(dataset_path):\n",
        "        emotion_path = os.path.join(dataset_path, emotion)\n",
        "        if os.path.isdir(emotion_path):\n",
        "            for file in os.listdir(emotion_path):\n",
        "                if file.endswith('.wav'):\n",
        "                    file_path = os.path.join(emotion_path, file)\n",
        "                    audio, sr = librosa.load(file_path, sr=None)  # Load audio file\n",
        "                    mfcc = librosa.feature.mfcc(y=audio, sr=sr, n_mfcc=40)  # Extract MFCC features\n",
        "                    mfcc_scaled = np.mean(mfcc.T, axis=0)  # Average MFCC coefficients\n",
        "                    features.append(mfcc_scaled)\n",
        "                    labels.append(emotion)\n",
        "    return np.array(features), np.array(labels)\n",
        "\n",
        "# Define the path to your dataset\n",
        "dataset_path = r'/content/drive/My Drive/FINAL_CODE_EMO_R_A ORIGINAL - Copy/DATASET/Tess/'\n",
        "\n",
        "# Load the dataset\n",
        "X, y = load_data(dataset_path)\n",
        "\n",
        "# Encode the labels into numerical format\n",
        "label_encoder = LabelEncoder()\n",
        "y_encoded = label_encoder.fit_transform(y)\n",
        "\n",
        "# Split the dataset into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=42)\n",
        "\n",
        "# Feature Standardization\n",
        "X_train = (X_train - np.mean(X_train, axis=0)) / np.std(X_train, axis=0)\n",
        "X_test = (X_test - np.mean(X_test, axis=0)) / np.std(X_test, axis=0)\n",
        "\n",
        "# Reshape the data to fit the input requirements of an RNN (LSTM layer)\n",
        "X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)\n",
        "X_test = X_test.reshape(X_test.shape[0], X_test.shape[1], 1)\n",
        "\n",
        "# Build the RNN model using LSTM layers\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.LSTM(128, return_sequences=True, input_shape=(X_train.shape[1], 1)),  # LSTM layer\n",
        "    tf.keras.layers.LSTM(128, return_sequences=False),  # LSTM layer\n",
        "    tf.keras.layers.Dropout(0.5),\n",
        "    tf.keras.layers.Dense(256, activation='relu'),\n",
        "    tf.keras.layers.Dense(len(label_encoder.classes_), activation='softmax')\n",
        "])\n",
        "\n",
        "# Compile the model with Adam optimizer and sparse categorical crossentropy loss\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Define EarlyStopping callback\n",
        "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=10, verbose=1, restore_best_weights=True)\n",
        "\n",
        "# Train the model on the training data\n",
        "model.fit(X_train, y_train, epochs=150, batch_size=64, validation_data=(X_test, y_test), callbacks=[early_stopping])\n",
        "\n",
        "# Save the trained model\n",
        "model.save('emotion_detection_model_rnn.h5')\n",
        "print('Model saved to disk.')\n",
        "\n",
        "# Save the label encoder\n",
        "label_encoder_path = 'label_encoder.pkl'\n",
        "joblib.dump(label_encoder, label_encoder_path)\n",
        "print(f'Label encoder saved to {label_encoder_path}.')\n",
        "\n",
        "# Evaluate the model on the test data\n",
        "y_pred = np.argmax(model.predict(X_test), axis=1)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f'Accuracy: {accuracy * 100:.2f}%')\n",
        "\n",
        "# Print the classification report\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred, target_names=label_encoder.classes_))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jj0xpgQA7rSX",
        "outputId": "17f2eef4-32cc-4ac8-ab18-8625559c2e30"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/150\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 260ms/step - accuracy: 0.7858 - loss: 0.6509 - val_accuracy: 0.9296 - val_loss: 0.4234\n",
            "Epoch 2/150\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 159ms/step - accuracy: 0.9521 - loss: 0.3260 - val_accuracy: 1.0000 - val_loss: 0.0926\n",
            "Epoch 3/150\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 165ms/step - accuracy: 1.0000 - loss: 0.0389 - val_accuracy: 1.0000 - val_loss: 0.0018\n",
            "Epoch 4/150\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 394ms/step - accuracy: 1.0000 - loss: 0.0020 - val_accuracy: 1.0000 - val_loss: 2.6993e-05\n",
            "Epoch 5/150\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 283ms/step - accuracy: 0.9982 - loss: 0.0023 - val_accuracy: 1.0000 - val_loss: 1.8901e-05\n",
            "Epoch 6/150\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 297ms/step - accuracy: 0.9960 - loss: 0.0078 - val_accuracy: 1.0000 - val_loss: 1.5481e-04\n",
            "Epoch 7/150\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 155ms/step - accuracy: 1.0000 - loss: 3.5166e-04 - val_accuracy: 1.0000 - val_loss: 8.8476e-04\n",
            "Epoch 8/150\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 172ms/step - accuracy: 0.9880 - loss: 0.0362 - val_accuracy: 1.0000 - val_loss: 1.1148e-05\n",
            "Epoch 9/150\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 163ms/step - accuracy: 1.0000 - loss: 4.7974e-06 - val_accuracy: 1.0000 - val_loss: 1.6504e-06\n",
            "Epoch 10/150\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 161ms/step - accuracy: 1.0000 - loss: 7.8789e-04 - val_accuracy: 1.0000 - val_loss: 2.0484e-07\n",
            "Epoch 11/150\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 154ms/step - accuracy: 1.0000 - loss: 4.8574e-04 - val_accuracy: 1.0000 - val_loss: 0.0030\n",
            "Epoch 12/150\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 212ms/step - accuracy: 1.0000 - loss: 0.0119 - val_accuracy: 1.0000 - val_loss: 2.3613e-04\n",
            "Epoch 12: early stopping\n",
            "Restoring model weights from the end of the best epoch: 2.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model saved to disk.\n",
            "Label encoder saved to label_encoder.pkl.\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 159ms/step\n",
            "Accuracy: 100.00%\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "   OAF_angry       1.00      1.00      1.00        31\n",
            "     YAF_sad       1.00      1.00      1.00        40\n",
            "\n",
            "    accuracy                           1.00        71\n",
            "   macro avg       1.00      1.00      1.00        71\n",
            "weighted avg       1.00      1.00      1.00        71\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import librosa\n",
        "import tensorflow as tf\n",
        "import joblib\n",
        "\n",
        "# Function to preprocess a single audio file\n",
        "def preprocess_audio(file_path):\n",
        "    audio, sr = librosa.load(file_path, sr=None)  # Load audio file\n",
        "    mfcc = librosa.feature.mfcc(y=audio, sr=sr, n_mfcc=40)  # Extract MFCC features\n",
        "    mfcc_scaled = np.mean(mfcc.T, axis=0)  # Average MFCC coefficients\n",
        "    return mfcc_scaled\n",
        "\n",
        "# Load the saved model\n",
        "model = tf.keras.models.load_model('emotion_detection_model_rnn.h5')\n",
        "print(\"Model loaded successfully.\")\n",
        "\n",
        "# Load the saved label encoder\n",
        "label_encoder = joblib.load('label_encoder.pkl')\n",
        "print(\"Label encoder loaded successfully.\")\n",
        "\n",
        "# Function to make predictions\n",
        "def predict_emotion(audio_file):\n",
        "    # Preprocess the audio file\n",
        "    features = preprocess_audio(audio_file)\n",
        "\n",
        "    # Standardize the features (assuming training mean/std was 0 and 1)\n",
        "    features = (features - np.mean(features)) / np.std(features)\n",
        "\n",
        "    # Reshape to match the model input\n",
        "    features = features.reshape(1, -1, 1)\n",
        "\n",
        "    # Make predictions\n",
        "    prediction = model.predict(features)\n",
        "    predicted_label_index = np.argmax(prediction)\n",
        "\n",
        "    # Decode the label\n",
        "    predicted_emotion = label_encoder.inverse_transform([predicted_label_index])[0]\n",
        "\n",
        "    return predicted_emotion\n",
        "\n",
        "# Test the prediction function with an example audio file\n",
        "audio_file_path = r'/content/drive/My Drive/FINAL_CODE_EMO_R_A ORIGINAL - Copy/DATASET/Tess/OAF_angry/OAF_back_angry.wav'\n",
        "predicted_emotion = predict_emotion(audio_file_path)\n",
        "\n",
        "print(f\"The predicted emotion for the audio file is: {predicted_emotion}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J1OawWyN7tQS",
        "outputId": "cc990fca-13e5-45a1-f3ce-078f19c07ad3"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model loaded successfully.\n",
            "Label encoder loaded successfully.\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 414ms/step\n",
            "The predicted emotion for the audio file is: OAF_angry\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Welcome To Colab",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}